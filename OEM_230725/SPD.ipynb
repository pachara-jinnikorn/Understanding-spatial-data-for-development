{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTGkOTdOpBll"
   },
   "source": [
    "# OpenEarhMap Semantinc Segmentation\n",
    "\n",
    "This demo code demonstrates training and testing of UNet-EfficientNet-B4 for the OpenEarthMap dataset (https://open-earth-map.org/). This demo code is based on the work from the \"segmentation_models.pytorch\" repository by qubvel, available at: https://github.com/qubvel/segmentation_models.pytorch. We extend our sincere appreciation to the original author for their invaluable contributions to the field of semantic segmentation and for providing this open-source implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWiUctcOpBlo"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17923,
     "status": "ok",
     "timestamp": 1690293306384,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "gQdFTlpypBlp",
    "outputId": "755dab4f-711e-4438-c42c-a1a5cbbeb130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: affine in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (24.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (2024.8.30)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (8.1.8)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (2.2.2)\n",
      "Requirement already satisfied: click-plugins in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (3.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from click>=4.0->rasterio) (0.4.6)\n",
      "Requirement already satisfied: pretrainedmodels in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (0.21.0)\n",
      "Requirement already satisfied: munch in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (4.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->pretrainedmodels) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->pretrainedmodels) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->pretrainedmodels) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n",
      "Requirement already satisfied: efficientnet_pytorch in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from efficientnet_pytorch) (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n",
      "Requirement already satisfied: timm in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.14)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (0.28.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (0.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->timm) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio \n",
    "!pip install pretrainedmodels \n",
    "!pip install efficientnet_pytorch \n",
    "!pip install timm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4elx5iipBlq"
   },
   "source": [
    "### Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 8920,
     "status": "ok",
     "timestamp": 1690293451338,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "zhMDpFqZpBlr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m#Neural Network คือ Layer ที่ใช้ในการคำนวณค่า\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader \u001b[38;5;66;03m#คือ Dataset object และลิสต์ของจำนวนข้อมูลที่ต้องการจะแบ่ง\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource\u001b[39;00m  \u001b[38;5;66;03m# Ensure 'source.py' is in the 'final_finish' directory\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msmp\u001b[39;00m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\OEM\\OEM_230725\\source\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unet\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\OEM\\OEM_230725\\source\\dataset.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrasterio\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset \u001b[38;5;28;01mas\u001b[39;00m BaseDataset\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m T\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_multiband\u001b[39m(path):\n\u001b[0;32m      9\u001b[0m     src \u001b[38;5;241m=\u001b[39m rasterio\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\OEM\\OEM_230725\\source\\transforms.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mA\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "import sys #library ที่จัดเตรียมฟังก์ชันและตัวแปรที่ใช้เพื่อจัดการกับส่วนต่างๆของ Python Runtime Environment\n",
    "sys.path.append(r'C:\\Users\\Admin\\Downloads\\OEM\\OEM_230725\\OEM_demo_teh.ipynb')  # Correct path to the directory containing source.py\n",
    "import os #นำเข้า module OS มาในโค้ดภาษา Python\n",
    "import time #คำสั่งต่างๆ มากมายที่เกี่ยวกับเวลา\n",
    "import numpy as np # library ที่ใช้ในการคำนวนทางคณิตศาสตร์ในภาษา Python\n",
    "import torch #library for use Machine Learning\n",
    "import torch.nn as nn #Neural Network คือ Layer ที่ใช้ในการคำนวณค่า\n",
    "from torch.utils.data import DataLoader #คือ Dataset object และลิสต์ของจำนวนข้อมูลที่ต้องการจะแบ่ง\n",
    "import source  # Ensure 'source.py' is in the 'final_finish' directory\n",
    "import segmentation_models_pytorch as smp #\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os768d4EpBlr"
   },
   "source": [
    "### Define main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1690293575342,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "fOj5_N4apBlr",
    "outputId": "fcfc3a93-3030-4614-ecdf-ba54ca866b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs   : 5\n",
      "Number of classes  : 9\n",
      "Batch size         : 4\n",
      "Device             : cuda\n"
     ]
    }
   ],
   "source": [
    "OEM_ROOT = r\"C:\\Users\\lucky\\Desktop\\final_finish\\OEM_230725\\data\\OpenEarthMap_Demo\"\n",
    "OEM_DATA_DIR = os.path.join(OEM_ROOT, 'train_val')\n",
    "TEST_DIR = OEM_ROOT+'test/'\n",
    "TRAIN_LIST = os.path.join(OEM_ROOT, \"train.txt\")\n",
    "VAL_LIST = os.path.join(OEM_ROOT, \"val.txt\")\n",
    "WEIGHT_DIR = r\"C:\\Users\\lucky\\Desktop\\final_finish\\OEM_230725\\weight\" # path to save weights\n",
    "OUT_DIR = r\"C:\\Users\\lucky\\Desktop\\final_finish\\OEM_230725\\result\" # path to save prediction images\n",
    "os.makedirs(WEIGHT_DIR, exist_ok=True)\n",
    "test_large = OEM_ROOT+'/cr2.tif'\n",
    "\n",
    "seed = 0\n",
    "learning_rate = 0.0001\n",
    "batch_size = 4\n",
    "n_epochs = 5\n",
    "classes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "n_classes = len(classes)+1\n",
    "classes_wt = np.ones([n_classes], dtype=np.float32)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Number of epochs   :\", n_epochs)\n",
    "print(\"Number of classes  :\", n_classes)\n",
    "print(\"Batch size         :\", batch_size)\n",
    "print(\"Device             :\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCETH4-PpBls"
   },
   "source": [
    "### Prepare training and validation file lists\n",
    "\n",
    "In this demo for Google Colab, we use only two regions, i.e., Tokyo and Kyoto for training. To train with the full set, please download the OpenEarthMap dataset from https://zenodo.org/record/7223446. Note for xBD data preparation is available at https://github.com/bao18/open_earth_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1690293462748,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "gn8aGxUvpBls",
    "outputId": "82f2abed-3597-4e31-abbf-a0c0c3d40804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples      : 98\n",
      "Training samples   : 84\n",
      "Validation samples : 14\n"
     ]
    }
   ],
   "source": [
    "img_pths = [f for f in Path(OEM_DATA_DIR).rglob(\"*.tif\") if \"labels\" in str(f)]\n",
    "train_pths = [str(f) for f in img_pths if f.name in np.loadtxt(TRAIN_LIST, dtype=str)]\n",
    "val_pths = [str(f) for f in img_pths if f.name in np.loadtxt(VAL_LIST, dtype=str)]\n",
    "\n",
    "print(\"Total samples      :\", len(img_pths))\n",
    "print(\"Training samples   :\", len(train_pths))\n",
    "print(\"Validation samples :\", len(val_pths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-uFSOcgpBlt"
   },
   "source": [
    "### Define training and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1690293465821,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "DkjJGjCOpBlt"
   },
   "outputs": [],
   "source": [
    "trainset = source.dataset.Dataset(train_pths, classes=classes, size=512, train=True)\n",
    "validset = source.dataset.Dataset(val_pths, classes=classes, train=False)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR9cFGaspBlt"
   },
   "source": [
    "### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5902,
     "status": "ok",
     "timestamp": 1690293476037,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "oTIQto7LpBlt",
    "outputId": "dceb44e7-4f97-4865-c153-aa65fe099f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\n",
      "Model output name  : u-efficientnet-b4_s0_CELoss\n",
      "Number of parameters:  20304278\n"
     ]
    }
   ],
   "source": [
    "network = smp.Unet(\n",
    "    classes=n_classes,\n",
    "    activation=None,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    decoder_attention_type=\"scse\",\n",
    ")\n",
    "\n",
    "# count parameters\n",
    "params = 0\n",
    "for p in network.parameters():\n",
    "    if p.requires_grad:\n",
    "        params += p.numel()\n",
    "\n",
    "criterion = source.losses.CEWithLogitsLoss(weights=classes_wt)\n",
    "criterion_name = 'CE'\n",
    "metric = source.metrics.IoU2()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "network_fout = f\"{network.name}_s{seed}_{criterion.name}\"\n",
    "OUT_DIR += network_fout # path to save prediction images\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Model output name  :\", network_fout)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Number of GPUs :\", torch.cuda.device_count())\n",
    "    network = torch.nn.DataParallel(network)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [dict(params=network.module.parameters(), lr=learning_rate)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2lQx068bQwo"
   },
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1690293490628,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "yTJFfnmspBlu"
   },
   "outputs": [],
   "source": [
    "class_rgb = {\n",
    "    \"Bareland\": [128, 0, 0],\n",
    "    \"Grass\": [0, 255, 36],\n",
    "    \"Pavement\": [148, 148, 148],\n",
    "    \"Road\": [255, 255, 255],\n",
    "    \"Tree\": [34, 97, 38],\n",
    "    \"Water\": [0, 69, 255],\n",
    "    \"Cropland\": [75, 181, 73],\n",
    "    \"buildings\": [222, 31, 7],\n",
    "}\n",
    "\n",
    "class_gray = {\n",
    "    \"Bareland\": 1,\n",
    "    \"Grass\": 2,\n",
    "    \"Pavement\": 3,\n",
    "    \"Road\": 4,\n",
    "    \"Tree\": 5,\n",
    "    \"Water\": 6,\n",
    "    \"Cropland\": 7,\n",
    "    \"buildings\": 8,\n",
    "}\n",
    "\n",
    "def label2rgb(a):\n",
    "    \"\"\"\n",
    "    a: labels (HxW)\n",
    "    \"\"\"\n",
    "    out = np.zeros(shape=a.shape + (3,), dtype=\"uint8\")\n",
    "    for k, v in class_gray.items():\n",
    "        out[a == v, 0] = class_rgb[k][0]\n",
    "        out[a == v, 1] = class_rgb[k][1]\n",
    "        out[a == v, 2] = class_rgb[k][2]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn9_yGisbYAN"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194067,
     "status": "ok",
     "timestamp": 1690291827169,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "1t5R7gDKpBlu",
    "outputId": "6a0789b0-3d62-47fa-953d-1b700d006045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 21/21 [01:47<00:00,  5.10s/it, CELoss=2.34, mIoU=0.0513]\n",
      "Valid: 100%|██████████| 4/4 [00:40<00:00, 10.24s/it, CELoss=2.08, mIoU=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 21/21 [01:46<00:00,  5.05s/it, CELoss=2.12, mIoU=0.0905]\n",
      "Valid: 100%|██████████| 4/4 [00:41<00:00, 10.25s/it, CELoss=2.08, mIoU=0.085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 21/21 [01:45<00:00,  5.03s/it, CELoss=1.97, mIoU=0.13] \n",
      "Valid: 100%|██████████| 4/4 [00:40<00:00, 10.23s/it, CELoss=2.08, mIoU=0.072] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 21/21 [01:45<00:00,  5.00s/it, CELoss=1.82, mIoU=0.164]\n",
      "Valid: 100%|██████████| 4/4 [00:40<00:00, 10.24s/it, CELoss=2.17, mIoU=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 21/21 [01:45<00:00,  5.01s/it, CELoss=1.68, mIoU=0.189]\n",
      "Valid: 100%|██████████| 4/4 [00:40<00:00, 10.22s/it, CELoss=2.24, mIoU=0.047] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 734.7135033607483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "max_score = 0\n",
    "train_hist = []\n",
    "valid_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  print(f\"\\nEpoch: {epoch + 1}\")\n",
    "\n",
    "  logs_train = source.runner.train_epoch(\n",
    "      model=network,\n",
    "      optimizer=optimizer,\n",
    "      criterion=criterion,\n",
    "      metric=metric,\n",
    "      dataloader=train_loader,\n",
    "      device=device,\n",
    "  )\n",
    "\n",
    "  logs_valid = source.runner.valid_epoch(\n",
    "      model=network,\n",
    "      criterion=criterion,\n",
    "      metric=metric,\n",
    "      dataloader=valid_loader,\n",
    "      device=device,\n",
    "  )\n",
    "\n",
    "  train_hist.append(logs_train)\n",
    "  valid_hist.append(logs_valid)\n",
    "\n",
    "  score = logs_valid[metric.name]\n",
    "\n",
    "  if max_score < score:\n",
    "      max_score = score\n",
    "      torch.save(network.state_dict(), os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\"))\n",
    "      print(\"Model saved!\")\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONWiNQMWAHP4"
   },
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9502,
     "status": "ok",
     "timestamp": 1690293506779,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "s0slbVQcmQlp",
    "outputId": "133da56c-c059-4abf-b290-8b5401ab62ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 0.33695197105407715\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}_pretrained.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "test_pths = glob.glob(TEST_DIR+\"/*.tif\")\n",
    "#testset = source.dataset.Dataset(test_pths, classes=classes, train=False)\n",
    "\n",
    "for fn_img in test_pths:\n",
    "  img = source.dataset.load_multiband(fn_img)\n",
    "  h, w = img.shape[:2]\n",
    "  power = math.ceil(np.log2(h) / np.log2(2))\n",
    "  shape = (2 ** power, 2 ** power)\n",
    "  img = cv2.resize(img, shape)\n",
    "\n",
    "  # test time augmentation\n",
    "  imgs = []\n",
    "  imgs.append(img.copy())\n",
    "  imgs.append(img[:, ::-1, :].copy())\n",
    "  imgs.append(img[::-1, :, :].copy())\n",
    "  imgs.append(img[::-1, ::-1, :].copy())\n",
    "\n",
    "  input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
    "\n",
    "  pred = []\n",
    "  with torch.no_grad():\n",
    "      msk = network(input)\n",
    "      msk = torch.softmax(msk[:, :, ...], dim=1)\n",
    "      msk = msk.cpu().numpy()\n",
    "      pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
    "  pred = pred.argmax(axis=0).astype(\"uint8\")\n",
    "  size = pred.shape[0:]\n",
    "  y_pr = cv2.resize(pred, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "  # save image as png\n",
    "  filename = os.path.splitext(os.path.basename(fn_img))[0]\n",
    "  y_pr_rgb = label2rgb(y_pr)\n",
    "  Image.fromarray(y_pr_rgb).save(os.path.join(OUT_DIR, filename+'_pr.png'))\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3I0hKRyHMl3"
   },
   "source": [
    "### Testing a model for a large Geotiff image\n",
    "\n",
    "A sample image is provided by the Geospatial Information Authority of Japan at https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88911,
     "status": "ok",
     "timestamp": 1690293670303,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "WguTdAFqHWAe",
    "outputId": "f33e8f67-8953-4fca-bc11-11b670003234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 23.941489458084106\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}_pretrained.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "# process large Geotiff image\n",
    "img0 = source.dataset.load_multiband(test_large)\n",
    "\n",
    "# get crs and transform\n",
    "crs, trans = source.dataset.get_crs(test_large)\n",
    "\n",
    "if img0.shape[2] > 3:\n",
    "    img0 = img0[:,:,[0,1,2]]\n",
    "height = img0.shape[0]\n",
    "width = img0.shape[1]\n",
    "band = img0.shape[2]\n",
    "\n",
    "patch_size = 512\n",
    "stride = 256\n",
    "C = int(np.ceil( (width - patch_size) / stride ) + 1)\n",
    "R = int(np.ceil( (height - patch_size) / stride ) + 1)\n",
    "\n",
    "# weight matrix B for avoiding boundaries of patches\n",
    "if patch_size > stride:\n",
    "    w = patch_size\n",
    "    s1 = stride\n",
    "    s2 = w - s1\n",
    "    d = 1/(1+s2)\n",
    "    B1 = np.ones((w,w))\n",
    "    B1[:,s1::] = np.dot(np.ones((w,1)),(-np.arange(1,s2+1)*d+1).reshape(1,s2))\n",
    "    B2 = np.flip(B1)\n",
    "    B3 = B1.T\n",
    "    B4 = np.flip(B3)\n",
    "    B = B1*B2*B3*B4\n",
    "else:\n",
    "    B = np.ones((w,w))\n",
    "\n",
    "img1 = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1),3))\n",
    "img1[0:height,0:width,:] = img0.copy()\n",
    "\n",
    "pred_all = np.zeros((9,patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
    "weight = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
    "\n",
    "for r in range(R):\n",
    "    for c in range(C):\n",
    "        img = img1[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size,:].copy().astype(np.float32)/255\n",
    "        imgs = []\n",
    "        imgs.append(img.copy())\n",
    "        imgs.append(img[:, ::-1, :].copy())\n",
    "        imgs.append(img[::-1, :, :].copy())\n",
    "        imgs.append(img[::-1, ::-1, :].copy())\n",
    "\n",
    "        input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
    "\n",
    "        pred = []\n",
    "        with torch.no_grad():\n",
    "            msk = network(input)\n",
    "            msk = torch.softmax(msk[:, :, ...], dim=1)\n",
    "            msk = msk.cpu().numpy()\n",
    "\n",
    "            pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
    "\n",
    "        pred_all[:,r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += pred.copy()*B\n",
    "        weight[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += B\n",
    "\n",
    "for b in range(9):\n",
    "    pred_all[b,:,:] = pred_all[b,:,:]/weight\n",
    "    if b == 0:\n",
    "        pred_all[b,:,:] = 0\n",
    "\n",
    "pred_all = pred_all.argmax(axis=0).astype(\"uint8\")\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(test_large))[0]\n",
    "pr_rgb = label2rgb(pred_all)\n",
    "Image.fromarray(pr_rgb[0:height,0:width,:]).save(os.path.join(OUT_DIR, filename+'_pr.png'))\n",
    "\n",
    "# save geotiff\n",
    "pr_rgb = np.transpose(pr_rgb[0:height,0:width,:], (2,0,1))\n",
    "source.dataset.save_img(os.path.join(OUT_DIR, filename+'_pr.tif'),pr_rgb,crs,trans)\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
