{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTGkOTdOpBll"
   },
   "source": [
    "# OpenEarhMap Semantinc Segmentation\n",
    "\n",
    "This demo code demonstrates training and testing of UNet-EfficientNet-B4 for the OpenEarthMap dataset (https://open-earth-map.org/). This demo code is based on the work from the \"segmentation_models.pytorch\" repository by qubvel, available at: https://github.com/qubvel/segmentation_models.pytorch. We extend our sincere appreciation to the original author for their invaluable contributions to the field of semantic segmentation and for providing this open-source implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWiUctcOpBlo"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17923,
     "status": "ok",
     "timestamp": 1690293306384,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "gQdFTlpypBlp",
    "outputId": "755dab4f-711e-4438-c42c-a1a5cbbeb130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.4.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: affine in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (25.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (2025.1.31)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (8.1.8)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (2.2.3)\n",
      "Requirement already satisfied: click-plugins in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rasterio) (3.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click>=4.0->rasterio) (0.4.6)\n",
      "Requirement already satisfied: pretrainedmodels in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: torch in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (0.21.0+cu118)\n",
      "Requirement already satisfied: munch in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (4.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretrainedmodels) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->pretrainedmodels) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->pretrainedmodels) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->pretrainedmodels) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->pretrainedmodels) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: efficientnet_pytorch in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from efficientnet_pytorch) (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: timm in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (0.21.0+cu118)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (0.29.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->timm) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\se\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rasterio \n",
    "%pip install pretrainedmodels \n",
    "%pip install efficientnet_pytorch \n",
    "%pip install timm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4elx5iipBlq"
   },
   "source": [
    "### Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "executionInfo": {
     "elapsed": 8920,
     "status": "ok",
     "timestamp": 1690293451338,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "zhMDpFqZpBlr"
   },
   "outputs": [],
   "source": [
    "import sys #library ที่จัดเตรียมฟังก์ชันและตัวแปรที่ใช้เพื่อจัดการกับส่วนต่างๆของ Python Runtime Environment\n",
    "sys.path.append(r'C:\\Users\\SE\\Desktop\\yesib\\OEM_230725\\SPD.ipynb')  # Correct path to the directory containing source.py\n",
    "import os #นำเข้า module OS มาในโค้ดภาษา Python\n",
    "import time #คำสั่งต่างๆ มากมายที่เกี่ยวกับเวลา\n",
    "import numpy as np # library ที่ใช้ในการคำนวนทางคณิตศาสตร์ในภาษา Python\n",
    "import torch #library for use Machine Learning\n",
    "import torch.nn as nn #Neural Network คือ Layer ที่ใช้ในการคำนวณค่า\n",
    "from torch.utils.data import DataLoader #คือ Dataset object และลิสต์ของจำนวนข้อมูลที่ต้องการจะแบ่ง\n",
    "import source  # Ensure 'source.py' is in the 'final_finish' directory\n",
    "import segmentation_models_pytorch as smp #\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os768d4EpBlr"
   },
   "source": [
    "### Define main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1690293575342,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "fOj5_N4apBlr",
    "outputId": "fcfc3a93-3030-4614-ecdf-ba54ca866b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs   : 3\n",
      "Number of classes  : 9\n",
      "Batch size         : 4\n",
      "Device             : cuda\n"
     ]
    }
   ],
   "source": [
    "OEM_ROOT = r\"C:\\Users\\SE\\Desktop\\yesib\\OEM_230725\\data\\OpenEarthMap_Demo\"\n",
    "OEM_DATA_DIR = os.path.join(OEM_ROOT, 'train_val')\n",
    "TEST_DIR = OEM_ROOT+'test/'\n",
    "TRAIN_LIST = os.path.join(OEM_ROOT, \"train.txt\")\n",
    "VAL_LIST = os.path.join(OEM_ROOT, \"val.txt\")\n",
    "WEIGHT_DIR = r\"C:\\Users\\SE\\Desktop\\yesib\\OEM_230725\\weight\" # path to save weights\n",
    "OUT_DIR = r\"C:\\Users\\SE\\Desktop\\yesib\\OEM_230725\\result\" # path to save prediction images\n",
    "os.makedirs(WEIGHT_DIR, exist_ok=True)\n",
    "test_large = OEM_ROOT+'/helloworld.tif'\n",
    "\n",
    "seed = 0\n",
    "learning_rate = 0.0001\n",
    "batch_size = 4\n",
    "n_epochs = 3\n",
    "classes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "n_classes = len(classes)+1\n",
    "classes_wt = np.ones([n_classes], dtype=np.float32)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(\"Number of epochs   :\", n_epochs)\n",
    "print(\"Number of classes  :\", n_classes)\n",
    "print(\"Batch size         :\", batch_size)\n",
    "print(\"Device             :\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCETH4-PpBls"
   },
   "source": [
    "### Prepare training and validation file lists\n",
    "\n",
    "In this demo for Google Colab, we use only two regions, i.e., Tokyo and Kyoto for training. To train with the full set, please download the OpenEarthMap dataset from https://zenodo.org/record/7223446. Note for xBD data preparation is available at https://github.com/bao18/open_earth_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1690293462748,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "gn8aGxUvpBls",
    "outputId": "82f2abed-3597-4e31-abbf-a0c0c3d40804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples      : 10001\n",
      "Training samples   : 9\n",
      "Validation samples : 1\n"
     ]
    }
   ],
   "source": [
    "img_pths = [f for f in Path(OEM_DATA_DIR).rglob(\"*.png\") if \"labels\" in str(f)]\n",
    "train_pths = [str(f) for f in img_pths if f.name in np.loadtxt(TRAIN_LIST, dtype=str)]\n",
    "val_pths = [str(f) for f in img_pths if f.name in np.loadtxt(VAL_LIST, dtype=str)]\n",
    "\n",
    "print(\"Total samples      :\", len(img_pths))\n",
    "print(\"Training samples   :\", len(train_pths))\n",
    "print(\"Validation samples :\", len(val_pths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-uFSOcgpBlt"
   },
   "source": [
    "### Define training and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1690293465821,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "DkjJGjCOpBlt"
   },
   "outputs": [],
   "source": [
    "trainset = source.dataset.Dataset(train_pths, classes=classes, size=1024, train=True)\n",
    "validset = source.dataset.Dataset(val_pths, classes=classes, train=False)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR9cFGaspBlt"
   },
   "source": [
    "### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5902,
     "status": "ok",
     "timestamp": 1690293476037,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "oTIQto7LpBlt",
    "outputId": "dceb44e7-4f97-4865-c153-aa65fe099f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n",
      "https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\n",
      "Model output name  : u-efficientnet-b4_s0_CELoss\n",
      "Number of parameters:  20304278\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Check the version of PyTorch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "\n",
    "\n",
    "\n",
    "network = smp.Unet(\n",
    "    classes=n_classes,\n",
    "    activation=None,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    decoder_attention_type=\"scse\",\n",
    ")\n",
    "\n",
    "# count parameters\n",
    "params = 0\n",
    "for p in network.parameters():\n",
    "    if p.requires_grad:\n",
    "        params += p.numel()\n",
    "\n",
    "criterion = source.losses.CEWithLogitsLoss(weights=classes_wt)\n",
    "criterion_name = 'CE'\n",
    "metric = source.metrics.IoU2()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "network_fout = f\"{network.name}_s{seed}_{criterion.name}\"\n",
    "OUT_DIR += network_fout # path to save prediction images\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Model output name  :\", network_fout)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Number of GPUs :\", torch.cuda.device_count())\n",
    "    network = torch.nn.DataParallel(network)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [dict(params=network.module.parameters(), lr=learning_rate)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2lQx068bQwo"
   },
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1690293490628,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "yTJFfnmspBlu"
   },
   "outputs": [],
   "source": [
    "class_rgb = {\n",
    "    \"Bareland\": [128, 0, 0],\n",
    "    \"Grass\": [0, 255, 36],\n",
    "    \"Pavement\": [148, 148, 148],\n",
    "    \"Road\": [255, 255, 255],\n",
    "    \"Tree\": [34, 97, 38],\n",
    "    \"Water\": [0, 69, 255],\n",
    "    \"Cropland\": [75, 181, 73],\n",
    "    \"buildings\": [222, 31, 7],\n",
    "}\n",
    "\n",
    "class_gray = {\n",
    "    \"Bareland\": 1,\n",
    "    \"Grass\": 2,\n",
    "    \"Pavement\": 3,\n",
    "    \"Road\": 4,\n",
    "    \"Tree\": 5,\n",
    "    \"Water\": 6,\n",
    "    \"Cropland\": 7,\n",
    "    \"buildings\": 8,\n",
    "}\n",
    "\n",
    "def label2rgb(a):\n",
    "    \"\"\"\n",
    "    a: labels (HxW)\n",
    "    \"\"\"\n",
    "    out = np.zeros(shape=a.shape + (3,), dtype=\"uint8\")\n",
    "    for k, v in class_gray.items():\n",
    "        out[a == v, 0] = class_rgb[k][0]\n",
    "        out[a == v, 1] = class_rgb[k][1]\n",
    "        out[a == v, 2] = class_rgb[k][2]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn9_yGisbYAN"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194067,
     "status": "ok",
     "timestamp": 1690291827169,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "1t5R7gDKpBlu",
    "outputId": "6a0789b0-3d62-47fa-953d-1b700d006045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resuming training from C:\\Users\\SE\\Desktop\\yesib\\OEM_230725\\weight\\u-efficientnet-b4_s0_CELoss.pth\n",
      "\n",
      "🚀 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:27<00:00,  9.33s/it, CELoss=2.41, mIoU=2.90%]\n",
      "Valid: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s, CELoss=2.33, mIoU=2.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model saved!\n",
      "\n",
      "📌 **IoU per class (Train & Valid) (as %):**\n",
      " - Class 0: Train IoU: 0.00%, Valid IoU: 0.00%\n",
      " - Class 1: Train IoU: 2.04%, Valid IoU: 7.76%\n",
      " - Class 2: Train IoU: 2.67%, Valid IoU: 2.70%\n",
      " - Class 3: Train IoU: 0.89%, Valid IoU: 1.38%\n",
      " - Class 4: Train IoU: 1.08%, Valid IoU: 4.29%\n",
      " - Class 5: Train IoU: 0.11%, Valid IoU: 0.04%\n",
      " - Class 6: Train IoU: 0.12%, Valid IoU: 0.04%\n",
      " - Class 7: Train IoU: 0.76%, Valid IoU: 0.97%\n",
      " - Class 8: Train IoU: 0.63%, Valid IoU: 6.28%\n",
      "\n",
      "🏆 **Max IoU Score (Validation): Class 2 with 7.76%**\n",
      "🏆 **Max IoU Score (Train): Class 3 with 2.67%**\n",
      "\n",
      "📌 **Epoch 1 Summary**\n",
      "🏆 Max Train mIoU so far: 2.90%\n",
      "🏆 Max Valid mIoU so far: 2.61%\n",
      "\n",
      "🚀 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:27<00:00,  9.14s/it, CELoss=2.36, mIoU=3.12%]\n",
      "Valid: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s, CELoss=2.34, mIoU=2.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model saved!\n",
      "\n",
      "📌 **IoU per class (Train & Valid) (as %):**\n",
      " - Class 0: Train IoU: 0.00%, Valid IoU: 0.00%\n",
      " - Class 1: Train IoU: 2.97%, Valid IoU: 7.36%\n",
      " - Class 2: Train IoU: 3.13%, Valid IoU: 2.69%\n",
      " - Class 3: Train IoU: 0.74%, Valid IoU: 1.40%\n",
      " - Class 4: Train IoU: 0.99%, Valid IoU: 4.56%\n",
      " - Class 5: Train IoU: 0.14%, Valid IoU: 0.06%\n",
      " - Class 6: Train IoU: 0.18%, Valid IoU: 0.03%\n",
      " - Class 7: Train IoU: 0.86%, Valid IoU: 1.04%\n",
      " - Class 8: Train IoU: 0.64%, Valid IoU: 9.50%\n",
      "\n",
      "🏆 **Max IoU Score (Validation): Class 9 with 9.50%**\n",
      "🏆 **Max IoU Score (Train): Class 3 with 3.13%**\n",
      "\n",
      "📌 **Epoch 2 Summary**\n",
      "🏆 Max Train mIoU so far: 3.12%\n",
      "🏆 Max Valid mIoU so far: 2.96%\n",
      "\n",
      "🚀 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:27<00:00,  9.15s/it, CELoss=2.31, mIoU=3.18%]\n",
      "Valid: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s, CELoss=2.31, mIoU=2.98%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model saved!\n",
      "\n",
      "📌 **IoU per class (Train & Valid) (as %):**\n",
      " - Class 0: Train IoU: 0.00%, Valid IoU: 0.00%\n",
      " - Class 1: Train IoU: 2.01%, Valid IoU: 6.82%\n",
      " - Class 2: Train IoU: 3.04%, Valid IoU: 3.09%\n",
      " - Class 3: Train IoU: 0.83%, Valid IoU: 1.65%\n",
      " - Class 4: Train IoU: 1.11%, Valid IoU: 4.52%\n",
      " - Class 5: Train IoU: 0.24%, Valid IoU: 0.07%\n",
      " - Class 6: Train IoU: 0.19%, Valid IoU: 0.03%\n",
      " - Class 7: Train IoU: 0.87%, Valid IoU: 1.20%\n",
      " - Class 8: Train IoU: 0.78%, Valid IoU: 9.41%\n",
      "\n",
      "🏆 **Max IoU Score (Validation): Class 9 with 9.41%**\n",
      "🏆 **Max IoU Score (Train): Class 3 with 3.04%**\n",
      "\n",
      "📌 **Epoch 3 Summary**\n",
      "🏆 Max Train mIoU so far: 3.18%\n",
      "🏆 Max Valid mIoU so far: 2.98%\n",
      "\n",
      "⏳ Processing time: 85.49411153793335\n",
      "🔄 Best Model loaded from C:\\Users\\SE\\Desktop\\yesib\\OEM_230725\\weight\\u-efficientnet-b4_s0_CELoss.pth\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "start = time.time()\n",
    "\n",
    "max_score = 0\n",
    "train_hist = []\n",
    "valid_hist = []\n",
    "best_model_path = os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\")\n",
    "\n",
    "# Load best model if it exists before training\n",
    "if os.path.exists(best_model_path):\n",
    "    network.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"🔄 Resuming training from {best_model_path}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\n🚀 Epoch: {epoch + 1}\")\n",
    "\n",
    "    logs_train, train_iou_per_class = source.runner.train_epoch(\n",
    "        model=network,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        metric=metric,\n",
    "        dataloader=train_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    logs_valid, valid_iou_per_class = source.runner.valid_epoch(\n",
    "        model=network,\n",
    "        criterion=criterion,\n",
    "        metric=metric,\n",
    "        dataloader=valid_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    train_hist.append(logs_train)\n",
    "    valid_hist.append(logs_valid)\n",
    "\n",
    "    score = logs_valid[\"mIoU\"]\n",
    "\n",
    "    if max_score < score:\n",
    "        max_score = score\n",
    "        torch.save(network.state_dict(), best_model_path)\n",
    "        print(\"✅ Best Model saved!\")\n",
    "\n",
    "    # Print per-class IoU for train and validation\n",
    "    print(\"\\n📌 **IoU per class (Train & Valid) (as %):**\")\n",
    "    for i, (train_iou, valid_iou) in enumerate(zip(train_iou_per_class, valid_iou_per_class), start=0):\n",
    "        print(f\" - Class {i}: Train IoU: {train_iou * 100:.2f}%, Valid IoU: {valid_iou * 100:.2f}%\")\n",
    "\n",
    "    # Find the best-performing class for validation\n",
    "    best_valid_class = np.nanargmax(valid_iou_per_class) + 1\n",
    "    best_valid_class_iou = valid_iou_per_class[best_valid_class - 1] * 100\n",
    "    print(f\"\\n🏆 **Max IoU Score (Validation): Class {best_valid_class} with {best_valid_class_iou:.2f}%**\")\n",
    "\n",
    "    # Find the best-performing class for training\n",
    "    best_train_class = np.nanargmax(train_iou_per_class) + 1\n",
    "    best_train_class_iou = train_iou_per_class[best_train_class - 1] * 100\n",
    "    print(f\"🏆 **Max IoU Score (Train): Class {best_train_class} with {best_train_class_iou:.2f}%**\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\n📌 **Epoch {epoch + 1} Summary**\")\n",
    "    print(f\"🏆 Max Train mIoU so far: {logs_train['mIoU'] * 100:.2f}%\")\n",
    "    print(f\"🏆 Max Valid mIoU so far: {logs_valid['mIoU'] * 100:.2f}%\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"\\n⏳ Processing time:\", end - start)\n",
    "\n",
    "# Load best model if needed\n",
    "if os.path.exists(best_model_path):\n",
    "    network.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"🔄 Best Model loaded from {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONWiNQMWAHP4"
   },
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9502,
     "status": "ok",
     "timestamp": 1690293506779,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "s0slbVQcmQlp",
    "outputId": "133da56c-c059-4abf-b290-8b5401ab62ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 0.11789298057556152\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "test_pths = glob.glob(TEST_DIR+\"/*.tif\")\n",
    "#testset = source.dataset.Dataset(test_pths, classes=classes, train=False)\n",
    "\n",
    "for fn_img in test_pths:\n",
    "  img = source.dataset.load_multiband(fn_img)\n",
    "  h, w = img.shape[:2]\n",
    "  power = math.ceil(np.log2(h) / np.log2(2))\n",
    "  shape = (2 ** power, 2 ** power)\n",
    "  img = cv2.resize(img, shape)\n",
    "\n",
    "  # test time augmentation\n",
    "  imgs = []\n",
    "  imgs.append(img.copy())\n",
    "  imgs.append(img[:, ::-1, :].copy())\n",
    "  imgs.append(img[::-1, :, :].copy())\n",
    "  imgs.append(img[::-1, ::-1, :].copy())\n",
    "\n",
    "  input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
    "\n",
    "  pred = []\n",
    "  with torch.no_grad():\n",
    "      msk = network(input)\n",
    "      msk = torch.softmax(msk[:, :, ...], dim=1)\n",
    "      msk = msk.cpu().numpy()\n",
    "      pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
    "  pred = pred.argmax(axis=0).astype(\"uint8\")\n",
    "  size = pred.shape[0:]\n",
    "  y_pr = cv2.resize(pred, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "  # save image as png\n",
    "  filename = os.path.splitext(os.path.basename(fn_img))[0]\n",
    "  y_pr_rgb = label2rgb(y_pr)\n",
    "  Image.fromarray(y_pr_rgb).save(os.path.join(OUT_DIR, filename+'test.png'))\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3I0hKRyHMl3"
   },
   "source": [
    "### Testing a model for a large Geotiff image\n",
    "\n",
    "A sample image is provided by the Geospatial Information Authority of Japan at https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88911,
     "status": "ok",
     "timestamp": 1690293670303,
     "user": {
      "displayName": "Naoto Yokoya",
      "userId": "10610249095566174844"
     },
     "user_tz": -540
    },
    "id": "WguTdAFqHWAe",
    "outputId": "f33e8f67-8953-4fca-bc11-11b670003234"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[231], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m     msk \u001b[38;5;241m=\u001b[39m network(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     59\u001b[0m     msk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(msk[:, :, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     msk \u001b[38;5;241m=\u001b[39m \u001b[43mmsk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     62\u001b[0m     pred \u001b[38;5;241m=\u001b[39m (msk[\u001b[38;5;241m0\u001b[39m, :, :, :] \u001b[38;5;241m+\u001b[39m msk[\u001b[38;5;241m1\u001b[39m, :, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m msk[\u001b[38;5;241m2\u001b[39m, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m+\u001b[39m msk[\u001b[38;5;241m3\u001b[39m, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     64\u001b[0m pred_all[:,r\u001b[38;5;241m*\u001b[39mstride:r\u001b[38;5;241m*\u001b[39mstride\u001b[38;5;241m+\u001b[39mpatch_size,c\u001b[38;5;241m*\u001b[39mstride:c\u001b[38;5;241m*\u001b[39mstride\u001b[38;5;241m+\u001b[39mpatch_size] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m*\u001b[39mB\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "# process large Geotiff image\n",
    "img0 = source.dataset.load_multiband(test_large)\n",
    "\n",
    "# get crs and transform\n",
    "crs, trans = source.dataset.get_crs(test_large)\n",
    "\n",
    "if img0.shape[2] > 3:\n",
    "    img0 = img0[:,:,[0,1,2]]\n",
    "height = img0.shape[0]\n",
    "width = img0.shape[1]\n",
    "band = img0.shape[2]\n",
    "\n",
    "patch_size = 1024\n",
    "stride = 256\n",
    "C = int(np.ceil( (width - patch_size) / stride ) + 1)\n",
    "R = int(np.ceil( (height - patch_size) / stride ) + 1)\n",
    "\n",
    "# weight matrix B for avoiding boundaries of patches\n",
    "if patch_size > stride:\n",
    "    w = patch_size\n",
    "    s1 = stride\n",
    "    s2 = w - s1\n",
    "    d = 1/(1+s2)\n",
    "    B1 = np.ones((w,w))\n",
    "    B1[:,s1::] = np.dot(np.ones((w,1)),(-np.arange(1,s2+1)*d+1).reshape(1,s2))\n",
    "    B2 = np.flip(B1)\n",
    "    B3 = B1.T\n",
    "    B4 = np.flip(B3)\n",
    "    B = B1*B2*B3*B4\n",
    "else:\n",
    "    B = np.ones((w,w))\n",
    "\n",
    "img1 = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1),3))\n",
    "img1[0:height,0:width,:] = img0.copy()\n",
    "\n",
    "pred_all = np.zeros((9,patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
    "weight = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
    "\n",
    "for r in range(R):\n",
    "    for c in range(C):\n",
    "        img = img1[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size,:].copy().astype(np.float32)/255\n",
    "        imgs = []\n",
    "        imgs.append(img.copy())\n",
    "        imgs.append(img[:, ::-1, :].copy())\n",
    "        imgs.append(img[::-1, :, :].copy())\n",
    "        imgs.append(img[::-1, ::-1, :].copy())\n",
    "\n",
    "        input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
    "\n",
    "        pred = []\n",
    "        with torch.no_grad():\n",
    "            msk = network(input)\n",
    "            msk = torch.softmax(msk[:, :, ...], dim=1)\n",
    "            msk = msk.cpu().numpy()\n",
    "\n",
    "            pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
    "\n",
    "        pred_all[:,r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += pred.copy()*B\n",
    "        weight[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += B\n",
    "\n",
    "for b in range(9):\n",
    "    pred_all[b,:,:] = pred_all[b,:,:]/weight\n",
    "    if b == 0:\n",
    "        pred_all[b,:,:] = 0\n",
    "\n",
    "pred_all = pred_all.argmax(axis=0).astype(\"uint8\")\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(test_large))[0]\n",
    "pr_rgb = label2rgb(pred_all)\n",
    "Image.fromarray(pr_rgb[0:height,0:width,:]).save(os.path.join(OUT_DIR, filename+'_pr.png'))\n",
    "\n",
    "# save geotiff\n",
    "pr_rgb = np.transpose(pr_rgb[0:height,0:width,:], (2,0,1))\n",
    "source.dataset.save_img(os.path.join(OUT_DIR, filename+'_pr.tif'),pr_rgb,crs,trans)\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
